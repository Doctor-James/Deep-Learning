# CS231n学习笔记

@powered by 邹佳吕

此笔记基于同济子豪兄的**精讲CS231N斯坦福计算机视觉公开课**

## 1.前言

由于笔者之前学过一部分（P1-P3），此笔记从P4：神经网络与反向传播开始

## 2.神经网络与反向传播

首先回顾一下P3讲的一些知识

### 2.1. 损失函数

#### 2.1.1. **SVM铰链损失函数**

![image-20211004181117241](/home/zjl/.config/Typora/typora-user-images/image-20211004181117241.png)

Li = max(错误类别的分数 - 正确类别的分数 + 1 , 0)

#### 2.1.2 交叉熵损失函数

softmax函数保证输出为概率（归一性，非负性）

![image-20211004163511486](/home/zjl/.config/Typora/typora-user-images/image-20211004163511486.png)

交叉熵损失函数：其梯度为真实概率和预测概率之间的差距

![image-20211019190136808](/home/zjl/.config/Typora/typora-user-images/image-20211019190136808.png)



![image-20211004163728727](/home/zjl/.config/Typora/typora-user-images/image-20211004163728727.png)

#### 2.1.3. **正则化（Regularization）**

为防止模型过拟合，提高模型的泛化能力，通常会在损失函数的后面添加一个正则化项。L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓【惩罚】是指对损失函数中的某些参数做一些限制

- **L1正则化**

  ![image-20211005122005833](/home/zjl/.config/Typora/typora-user-images/image-20211005122005833.png)

比原始的更新规则多出了 η λ s g n ( θ )这一项。当 θ为正时，更新后的 θ变小。当 θ 为负时，更新后的 θ 变大——因此它的效果就是让 η 往0靠，使网络中的权重尽可能为0，也就相当于减小了网络复杂度，防止过拟合。

- **L2正则化**

  ![image-20211005122301108](/home/zjl/.config/Typora/typora-user-images/image-20211005122301108.png)

  在不使用L2正则化时，求导结果中 θ 前系数为1，现在 θ前面系数为 1 − η λ ，因为η、λ都是正的，所以 1 − η λ 小于1，它的效果是减小 θ ，这也就是权重衰减（weight decay）的由来。
  

### 2.2. 激活函数

- 输入值如果较大，则激活，较小，则抑制

- 使线性分类器产生非线性输出，否则堆多少层神经元都可用一个等效的单层网络替代

  ![image-20211005212409481](/home/zjl/.config/Typora/typora-user-images/image-20211005212409481.png)

  ### 1. sigmoid

  ![image-20211007193044873](/home/zjl/.config/Typora/typora-user-images/image-20211007193044873.png)

  3 problems：

  1. 饱和导致梯度消失
  2. 输出都是正数，不关于零点对称（zig zag path）
  3. 指数运算消耗计算资源

  ### 2. tanh（x）

  ![image-20211007193252989](/home/zjl/.config/Typora/typora-user-images/image-20211007193252989.png)

  ### 3. ReLU

  ![image-20211007193349794](/home/zjl/.config/Typora/typora-user-images/image-20211007193349794.png)

  优点：

  1. 不会饱和

  2. 容易计算

  3. 收敛速度快

  缺点：

  1. 不关于零点对称
  2. x<0时梯度为0（使得有些神经元“死掉”）（dead ReLU）

  ### 4. Leaky ReLU&&ELU

  ![image-20211007193727667](/home/zjl/.config/Typora/typora-user-images/image-20211007193727667.png)

  ![image-20211007193755404](/home/zjl/.config/Typora/typora-user-images/image-20211007193755404.png)

  ### 5. Maxout

  ![image-20211007193843258](/home/zjl/.config/Typora/typora-user-images/image-20211007193843258.png)

  ## 激活函数总结：

  ![image-20211007193934758](/home/zjl/.config/Typora/typora-user-images/image-20211007193934758.png)

### 2.3. 反向传播



![image-20211005213309464](/home/zjl/.config/Typora/typora-user-images/image-20211005213309464.png)

- 先求局部梯度，再相乘求全局梯度

  ![image-20211005213441872](/home/zjl/.config/Typora/typora-user-images/image-20211005213441872.png)

- 乘法交换梯度，加法平分梯度

  ![image-20211005213503212](/home/zjl/.config/Typora/typora-user-images/image-20211005213503212.png)

  

## 3. 卷积神经网络（CNN）

### 3.1. **卷积**

同一个卷积核对一张图片进行卷积，得到一个feature map，不同卷积核得到的feature map堆叠起来成为activation maps，作为下一层卷积的输入

![image-20211007160048620](/home/zjl/.config/Typora/typora-user-images/image-20211007160048620.png)

![image-20211007174426658](/home/zjl/.config/Typora/typora-user-images/image-20211007174426658.png)

- padding

  为了防止边缘像素只被进行了一次操作，而中间像素进行了多次操作，在原图像边缘补0

  ![image-20211007160231306](/home/zjl/.config/Typora/typora-user-images/image-20211007160231306.png)

- 多通道图像的卷积

  ![image-20211007160404083](/home/zjl/.config/Typora/typora-user-images/image-20211007160404083.png)

- 卷积的作用

  把原图中符合卷积核特征的特征提取出来

![image-20211007160838195](/home/zjl/.config/Typora/typora-user-images/image-20211007160838195.png)

### 3.2. 池化（降采样）

- 引入平移不变性
- 减少参数量
- 防止过拟合

![image-20211007174625585](/home/zjl/.config/Typora/typora-user-images/image-20211007174625585.png)

### 3.3. LeNet - 5

![image-20211007175207370](/home/zjl/.config/Typora/typora-user-images/image-20211007175207370.png)

![image-20211007175325572](/home/zjl/.config/Typora/typora-user-images/image-20211007175325572.png)

### 3.4. 1X1卷积的作用

![image-20211007180827297](/home/zjl/.config/Typora/typora-user-images/image-20211007180827297.png)

在边缘计算设备，移动设备中，轻量级模型需求较大（数据量尽量小）

***更小的卷积核，更深的网络，尽量避免池化层和全连接层***

## 4. 训练神经网络：

### 4.1. Data Preprocessing

![image-20211007194547223](/home/zjl/.config/Typora/typora-user-images/image-20211007194547223.png)

![image-20211007194601536](/home/zjl/.config/Typora/typora-user-images/image-20211007194601536.png)

![image-20211007200941788](/home/zjl/.config/Typora/typora-user-images/image-20211007200941788.png)

标准化处理之后：损失函数对W的微小改变不那么敏感，更容易优化，可以选择更大的学习率，不那么容易震荡

***一些数据预处理方法***

![image-20211007201148423](/home/zjl/.config/Typora/typora-user-images/image-20211007201148423.png)

### 4.2. 权重的初始化

权重初始化为同一个常数：

![image-20211007201410626](/home/zjl/.config/Typora/typora-user-images/image-20211007201410626.png)

1. 标准正态分布随机初始化权重

   **标准正态分布+tanh**

![image-20211007201857926](/home/zjl/.config/Typora/typora-user-images/image-20211007201857926.png)

![image-20211007201912292](/home/zjl/.config/Typora/typora-user-images/image-20211007201912292.png)

会出现梯度消失现象

2. Xavier（哈维尔）初始化

   **Xavier+tanh**

   ![image-20211007202040958](/home/zjl/.config/Typora/typora-user-images/image-20211007202040958.png)

   **Xavier+ReLU**

   ![image-20211007202336267](/home/zjl/.config/Typora/typora-user-images/image-20211007202336267.png)

   **Kaiming+ReLU**

   ![image-20211007202455536](/home/zjl/.config/Typora/typora-user-images/image-20211007202455536.png)

### 4.3 Batch Normalization

希望中间结果服从标准正态分布，可以保留梯度

![image-20211007203144412](/home/zjl/.config/Typora/typora-user-images/image-20211007203144412.png)

N：batch size

![image-20211007203215298](/home/zjl/.config/Typora/typora-user-images/image-20211007203215298.png)

训练阶段的Batch Normalization：把中间结果尽量的拉平，使得梯度尽量暴露出来

测试阶段，Batch Size = 1，用训练时总均值代替mini-batch的均值，用训练时总方差代替mini-batch的方差

**作用：**

![image-20211007203737748](/home/zjl/.config/Typora/typora-user-images/image-20211007203737748.png)

其他Normalization

![image-20211010114508734](/home/zjl/.config/Typora/typora-user-images/image-20211010114508734.png)

### 4.4 Optimizers（梯度下降优化器）

![image-20211010115226386](/home/zjl/.config/Typora/typora-user-images/image-20211010115226386.png)

![image-20211010115250273](/home/zjl/.config/Typora/typora-user-images/image-20211010115250273.png)

#### 4.4.1 传统随机梯度下降法（SGD）的缺点：

1. 在梯度较大的方向上容易发生震荡，不能通过减小学习率很好的解决
2. 陷入局部最优点，鞍点（梯度为0，高维空间中更普遍）

#### 4.4.2 SGD + Momentum

![image-20211010120017423](/home/zjl/.config/Typora/typora-user-images/image-20211010120017423.png)

1. 可摆脱鞍点和局部最优点（可理解为冲过了）
2. 不会在梯度较大的方向上容易发生震荡（震荡时，每次dx正负号相反，抵消掉）

#### 4.4.3 NAG

![image-20211010120653243](/home/zjl/.config/Typora/typora-user-images/image-20211010120653243.png)

#### 4.4.4 AdaGrad

![image-20211010121040952](/home/zjl/.config/Typora/typora-user-images/image-20211010121040952.png)

因为SGD会出现梯度较大的地方震荡，则加入一个惩罚项==grad_squared==,并且为累加，此惩罚项越来越大。

问题在于，惩罚项越来越大，更新量越来越小，可能衰减到0

#### 4.4.5 RMSProp

![image-20211010121319943](/home/zjl/.config/Typora/typora-user-images/image-20211010121319943.png)

***AdaGrad***的优化版本

将==grad_squared==引入衰减因子==decay_rate==,表示之前所有惩罚向对当前惩罚项作用的比重

#### 4.4.6 Adam

![image-20211010121729691](/home/zjl/.config/Typora/typora-user-images/image-20211010121729691.png)

综合上述两种优化器，引入第一动量和第二动量的概念

### 4.5 学习率

![image-20211011213215502](/home/zjl/.config/Typora/typora-user-images/image-20211011213215502.png)

学习率变更方法：先给大学习率，训练轮次越多，学习率越小

### 4.6 二阶优化算法

#### 4.6.1 牛顿法

![image-20211011213656805](/home/zjl/.config/Typora/typora-user-images/image-20211011213656805.png)

由于海森矩阵的逆矩阵难以求得，一般不用此方法

#### 4.6.2 拟牛顿法（BGFS）

### 4.7 过拟合

![image-20211011214045863](/home/zjl/.config/Typora/typora-user-images/image-20211011214045863.png)

![image-20211011214147460](/home/zjl/.config/Typora/typora-user-images/image-20211011214147460.png)

训练集准确率持续上涨，验证集准确率遇到瓶颈甚至下降。需要停掉训练（早停）

### 4.8 模型集成

1. 好而不同的模型集成
2. 同一个模型，不同轮数的模型集成

### 4.9 Dropout

每一轮随机杀死一部分神经元

防止过拟合，实现正则化

1. 打破神经元的联合适应性，减小神经元之间的依赖性，使其“独当一面”
2. 起到了模型集成的作用

![image-20211012232741115](/home/zjl/.config/Typora/typora-user-images/image-20211012232741115.png)

### 4.9 数据增强

旋转，剪切等，扩充原始数据集，防止过拟合，实现正则化

## 5 . 迁移学习

### 5.1 原理

![image-20211019153959339](/home/zjl/.config/Typora/typora-user-images/image-20211019153959339.png)

用预训练模型抽取图像特征（前面的层数冻结不动），在最后一层或几层训练

迁移学习策略

![image-20211019154417019](/home/zjl/.config/Typora/typora-user-images/image-20211019154417019.png)

### 5.2  数据集太小怎么破

1. 找一个类似的大数据集，训练模型
2. 对模型进行迁移学习和微调，泛化到小数据集上

## 6. 经典卷积神经网络结构案例分析

### 6.1 LeNet-5（1998）

手写数字识别（6万个参数）

![image-20211019160407586](/home/zjl/.config/Typora/typora-user-images/image-20211019160407586.png)

### 6.2 AlexNet（2012）

6千万个参数

创新点：

![image-20211020213346293](/home/zjl/.config/Typora/typora-user-images/image-20211020213346293.png)

![image-20211020213900017](/home/zjl/.config/Typora/typora-user-images/image-20211020213900017.png)

**两个GPU 模型并行：**

![image-20211020213627953](/home/zjl/.config/Typora/typora-user-images/image-20211020213627953.png)

### 6.3 ZFNet（2013）

![image-20211020213928941](/home/zjl/.config/Typora/typora-user-images/image-20211020213928941.png)

模型与AlexNet区别不大，贡献在于论文里的可视化

### 6.4 VGG （2014）

2014

![image-20211020214154198](/home/zjl/.config/Typora/typora-user-images/image-20211020214154198.png)

所有卷积核都是3X3，步长为1，padding为1（3个3X3的感受野与1个7X7一样，可代替，实现更少的参数量，更深的层数）

### 6.5 GoogLeNet（2014）

![image-20211021164614750](/home/zjl/.config/Typora/typora-user-images/image-20211021164614750.png)

用不同尺度的卷积核进行卷积，把结果堆叠起来，保证不管目标在图像中所占面积比例大还是小，均可将目标特征提取出来

![image-20211021163754462](/home/zjl/.config/Typora/typora-user-images/image-20211021163754462.png)

问题：运算量太大

![image-20211021163952863](/home/zjl/.config/Typora/typora-user-images/image-20211021163952863.png)

解决方法：使用1X1卷积核进行降维

### 6.6 ResNet（2015）

何恺明，吊打一切，图像分类准确率首次超过人类

![image-20211021164823068](/home/zjl/.config/Typora/typora-user-images/image-20211021164823068.png)

过深的网络会产生梯度消失现象，网络退化，使得准确率甚至低于浅层网络

**残差网络**

![image-20211021165050851](/home/zjl/.config/Typora/typora-user-images/image-20211021165050851.png)

### 6.7 算法比较

![image-20211021165818500](/home/zjl/.config/Typora/typora-user-images/image-20211021165818500.png)

**迁移学习推荐**：ResNet-18，ResNet-50，inception-v3

![image-20211021165922412](/home/zjl/.config/Typora/typora-user-images/image-20211021165922412.png)

### 6.8 NAS（神经架构搜索）

让网络自己设计网络

## 7 . 硬件算力基础

### 7.1 CPU vs GPU

![image-20211019161750797](/home/zjl/.config/Typora/typora-user-images/image-20211019161750797.png)

**CPU**：适合流水线任务，解决串行计算任务

**GPU**：大量慢速计算单元，适合解决并行任务

![image-20211019162240454](/home/zjl/.config/Typora/typora-user-images/image-20211019162240454.png)



![image-20211019163139649](/home/zjl/.config/Typora/typora-user-images/image-20211019163139649.png)

### 7.2 GPU

![image-20211019162443024](/home/zjl/.config/Typora/typora-user-images/image-20211019162443024.png)

### 7.3 计算瓶颈

硬盘（数据集）与GPU（权重）读写数据成为计算瓶颈

**解决方案**：

1. 将所有数据读入内存
2. 用固态硬盘替换机械硬盘
3. 使用多个CPU线程读取数据

### 7.4 TPU

专门为深度学习设计的硬件架构

### 7.5 inter神经棒

## 8 . 加速深度学习计算的算法

### 8.1 加速推断的算法

![image-20211019172355256](/home/zjl/.config/Typora/typora-user-images/image-20211019172355256.png)

#### 8.1.1 Pruning（剪枝）

![image-20211019170829426](/home/zjl/.config/Typora/typora-user-images/image-20211019170829426.png)

![image-20211019170844218](/home/zjl/.config/Typora/typora-user-images/image-20211019170844218.png)

剪取一些神经元（权重的绝对值接近0），再训练，在模型精度下降不大的情况下，可进行模型压缩

![image-20211019170927830](/home/zjl/.config/Typora/typora-user-images/image-20211019170927830.png)

#### 8.1.2 Trained Quantization（权重合并）

例如2附近的均用2表示（降低浮点数精度）

![image-20211019171600594](/home/zjl/.config/Typora/typora-user-images/image-20211019171600594.png)

![image-20211019171649703](/home/zjl/.config/Typora/typora-user-images/image-20211019171649703.png)

![image-20211019171930145](/home/zjl/.config/Typora/typora-user-images/image-20211019171930145.png)

SqueezeNet

![image-20211019172324622](/home/zjl/.config/Typora/typora-user-images/image-20211019172324622.png)

#### 8.1.3 Quantization（权重量化）

将权重和激活值最大最小值找到，区间等分为256个区间，用8bit整数表示（本质也是用更少的bit表示权重和激活值）

### 8.2 加速训练的算法

![image-20211019184748203](/home/zjl/.config/Typora/typora-user-images/image-20211019184748203.png)

#### 8.2.1 Parallelization（并行运算）

1. 数据并行
2. 模型并行

#### 8.2.2 Mixed Precision Training

![image-20211019185049058](/home/zjl/.config/Typora/typora-user-images/image-20211019185049058.png)

#### 8.2.3 Model Distillation (知识蒸馏)

![image-20211019185352645](/home/zjl/.config/Typora/typora-user-images/image-20211019185352645.png)

![image-20211019185408252](/home/zjl/.config/Typora/typora-user-images/image-20211019185408252.png)

凸显相似标签之间的关系（不是全令为0）

![image-20211019185543325](/home/zjl/.config/Typora/typora-user-images/image-20211019185543325.png)

## 9. 深度学习框架

![image-20211019231917070](/home/zjl/.config/Typora/typora-user-images/image-20211019231917070.png)

**深度学习框架的作用**

1. 快速验证模型
2. 构建计算图，自动求导
3. 在GPU上快速前向反向运算

### 9.1 Pytorch

Numpy + 自动求导 + GPU

![image-20211020210405352](/home/zjl/.config/Typora/typora-user-images/image-20211020210405352.png)

![image-20211020210429793](/home/zjl/.config/Typora/typora-user-images/image-20211020210429793.png)

#### 9.1.1Autograd（自动求导）

#### 9.1.2 nn模块

更高级的封装

![image-20211020211419950](/home/zjl/.config/Typora/typora-user-images/image-20211020211419950.png)

#### 9.1.3 optiom

优化器模块

![image-20211020211510268](/home/zjl/.config/Typora/typora-user-images/image-20211020211510268.png)

#### 9.1.4 DataLoaders

![image-20211020211621296](/home/zjl/.config/Typora/typora-user-images/image-20211020211621296.png)

#### 9.1.5 动态计算图

每次迭代从头重新构建一个计算图

### 9.2 TensorFlow

#### 9.2.1 静态图

构建一个计算图，每次迭代复用计算图

![image-20211020211934492](/home/zjl/.config/Typora/typora-user-images/image-20211020211934492.png)

### 9.3 Keras（高层封装库）

## 10 . 循环神经网络（RNN）

专门用来处理序列数据（自然语言处理，视频分类，语音识别，文本情感分析）

 ![image-20211021175901317](/home/zjl/.config/Typora/typora-user-images/image-20211021175901317.png)

每一个时刻使用同一个权值矩阵（权值共享）

![image-20211021180012310](/home/zjl/.config/Typora/typora-user-images/image-20211021180012310.png)

![image-20211021180055020](/home/zjl/.config/Typora/typora-user-images/image-20211021180055020.png)
